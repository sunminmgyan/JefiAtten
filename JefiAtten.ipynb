{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb54fd95-67f1-4bfe-a9b0-8ca62fabf9b0",
   "metadata": {},
   "source": [
    "# Sovling the Maxwell's equation using the attention mechnism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dc130-b4d7-414e-8deb-e159a6ee331c",
   "metadata": {},
   "source": [
    "We use both the encoder and decoder in transformer, published in paper \n",
    "\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98aafb-0788-44e5-b11a-18bd16a48dfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "The encoder is used as an embeding block, which converts the 3D rho and J vector at a certain time, e.g., shape [batch_size, num_steps, 8, nx ,ny, nz], into **a** embeded_source_vector. Here, the self-attention mechnism is used. **Note that this procedure can be replaced by the 3D convolutional layers in principle.**\n",
    "\n",
    "Similarly, the corresponding electormagnetic fields of shape [batch_size, num_steps, 10, nx ,ny, nz] are converted into **a** embeded_EM_vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf40b1-a28d-456a-a342-1e6bcd0e5a8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Define the package to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8e03f2-621c-4635-9d26-6e103b995302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangjunjie/anaconda3/envs/LLM/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, math\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad9e53-7b31-45ac-866d-eaf4ba205f0e",
   "metadata": {},
   "source": [
    "## 2. The JefiAtten blokcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2b0ac-3684-4016-a6c3-eb5846e8b4c0",
   "metadata": {},
   "source": [
    "### 2.1 Self-attention and Cross-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b30789b-272e-4018-b66e-425d358d7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerSelfAttention(nn.Module):\n",
    "    def __init__(self, query_EM_dim, mem_source_dim, embeded_dim, num_heads, num_layers,dropout):\n",
    "   \n",
    "        super(MultiLayerSelfAttention, self).__init__()\n",
    "    \n",
    "        self.num_heads = num_heads\n",
    "        self.embeded_dim = embeded_dim\n",
    "        self.MHembeding = self.num_heads*self.embeded_dim\n",
    "        \n",
    "        self.self_atten_linear_1 = nn.Linear(embeded_dim, embeded_dim * 4)\n",
    "        self.self_atten_linear_2 = nn.Linear(embeded_dim * 4, embeded_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(embeded_dim, dtype=torch.float32))\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm(self.embeded_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(self.embeded_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, n_batch):\n",
    "        scores = torch.einsum('bijkhd,bijkhd->bijkh', x, x) / self.sqrt_d\n",
    "        # softmax at the dimensions [i,j,k]\n",
    "        weights = torch.softmax(scores.view(n_batch, -1 ,self.num_heads), dim=1).view_as(scores)\n",
    "        \n",
    "        x = x + self.dropout(torch.einsum('bijkh,bijkhd->bijkhd', weights, x))\n",
    "        x = x + self.dropout(self.self_atten_linear_2(self.relu(self.self_atten_linear_1(x))))\n",
    "        \n",
    "        return x\n",
    "     \n",
    "    \n",
    "class MultiLayerAttention(nn.Module):\n",
    "    def __init__(self, query_EM_dim, mem_source_dim, embeded_dim, num_heads, num_layers, dropout):\n",
    "   \n",
    "        super(MultiLayerAttention, self).__init__()\n",
    "    \n",
    "        self.num_heads = num_heads\n",
    "        self.embeded_dim = embeded_dim\n",
    "        self.MHembeding = self.num_heads*self.embeded_dim\n",
    "        \n",
    "        self.atten_linear_1 = nn.Linear(embeded_dim, embeded_dim * 4)\n",
    "        self.atten_linear_2 = nn.Linear(embeded_dim * 4, embeded_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.linear = nn.Linear(self.MHembeding, embeded_dim)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(embeded_dim, dtype=torch.float32))\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm(self.embeded_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(self.embeded_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, k, q, v, n_batch):\n",
    "        # perform attention along the [nx, ny, nz] dimension\n",
    "        scores = torch.einsum('bijkhd,bijkhd->bijkh', q, k) / self.sqrt_d\n",
    "        # softmax at the dimensions [i,j,k]\n",
    "        weights = torch.softmax(scores.reshape(n_batch, -1, self.num_heads), dim=1).view_as(scores)\n",
    "        v = v + self.dropout(torch.einsum('bijkh,bijkhd->bijkhd', weights, v))\n",
    "        v = self.layer_norm_2(v + self.dropout(self.atten_linear_2(self.relu(self.atten_linear_1(v)))))\n",
    "        \n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f088d-8e65-42f4-bce9-60da4a97b1de",
   "metadata": {},
   "source": [
    "### 2.2 Time-attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef6fb9-55fd-4cba-945f-a56dc3bb2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSelfAttention(nn.Module):\n",
    "    def __init__(self, rho_J_dim, EM_dim, embeded_dim, dropout):\n",
    "   \n",
    "        super(TimeSelfAttention, self).__init__()\n",
    "        \n",
    "        self.embeded_dim = embeded_dim\n",
    "        self.rho_J_dim = rho_J_dim\n",
    "        self.EM_dim = EM_dim\n",
    "        \n",
    "        self.self_atten_linear_1 = nn.Linear(embeded_dim, embeded_dim * 4)\n",
    "        self.self_atten_linear_2 = nn.Linear(embeded_dim * 4, embeded_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(self.embeded_dim, dtype=torch.float32))\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm(self.embeded_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(self.embeded_dim)     \n",
    "        \n",
    "        self.linear_rho = nn.Linear(self.embeded_dim * self.rho_J_dim, self.rho_J_dim)\n",
    "        self.linear_em = nn.Linear(self.embeded_dim * self.EM_dim, self.EM_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, n_batch):\n",
    "        \n",
    "        scores = torch.einsum('bijkht,bijkht->bijkh', x, x) / self.sqrt_d\n",
    "        weights = torch.softmax(scores.view(n_batch, -1 ,self.embeded_dim), dim=1).view_as(scores)\n",
    "        \n",
    "        x = x + self.dropout(torch.einsum('bijkh,bijkht->bijkht', weights, x))\n",
    "        x = x + self.dropout(self.self_atten_linear_2(self.relu(self.self_atten_linear_1(x))))\n",
    "        if x.shape[4] == self.rho_J_dim:\n",
    "            out = self.linear_rho(x.reshape(*x.shape[:-2], -1))\n",
    "        else:\n",
    "            out = self.linear_em(x.reshape(*x.shape[:-2], -1))\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfed219-8e4c-4257-ab00-04c7f25119d6",
   "metadata": {},
   "source": [
    "### 2.3 Local-attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d18b972c-955a-4926-98dc-1c5ce9f5e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalrhoSelfAttention(nn.Module):\n",
    "    def __init__(self, rho_J_dim, dropout):\n",
    "   \n",
    "        super(LocalrhoSelfAttention, self).__init__()\n",
    "        \n",
    "        self.embeded_dim = rho_J_dim\n",
    "        \n",
    "        self.self_atten_linear_1 = nn.Linear(self.embeded_dim, self.embeded_dim * 4)\n",
    "        self.self_atten_linear_2 = nn.Linear(self.embeded_dim * 4, self.embeded_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(self.embeded_dim, dtype=torch.float32))\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm(self.embeded_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(self.embeded_dim)     \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, k,q,v, n_batch):\n",
    "        \n",
    "        scores = torch.einsum('bijkh,bijkh->bijkh', q, k) / self.sqrt_d\n",
    "        weights = torch.softmax(scores.view(n_batch, -1 ,self.embeded_dim), dim=1).view_as(scores)\n",
    "        \n",
    "        v = v + self.dropout(torch.einsum('bijkh,bijkh->bijkh', weights, v))\n",
    "        v = v + self.dropout(self.self_atten_linear_2(self.relu(self.self_atten_linear_1(v))))\n",
    "        return v \n",
    "    \n",
    "class LocalemSelfAttention(nn.Module):\n",
    "    def __init__(self, EM_dim, dropout):\n",
    "   \n",
    "        super(LocalemSelfAttention, self).__init__()\n",
    "        \n",
    "        self.embeded_dim = EM_dim\n",
    "        \n",
    "        self.self_atten_linear_1 = nn.Linear(self.embeded_dim, self.embeded_dim * 4)\n",
    "        self.self_atten_linear_2 = nn.Linear(self.embeded_dim * 4, self.embeded_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(self.embeded_dim, dtype=torch.float32))\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm(self.embeded_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(self.embeded_dim)     \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, k,q,v, n_batch):\n",
    "        \n",
    "        scores = torch.einsum('bijkh,bijkh->bijkh', q, k) / self.sqrt_d\n",
    "        weights = torch.softmax(scores.view(n_batch, -1 ,self.embeded_dim), dim=1).view_as(scores)\n",
    "        \n",
    "        v = v + self.dropout(torch.einsum('bijkh,bijkh->bijkh', weights, v))\n",
    "        v = v + self.dropout(self.self_atten_linear_2(self.relu(self.self_atten_linear_1(v))))\n",
    "        return v    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134221f4-183a-43da-8577-4abc2ca236b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid_size_o, y_grid_size_o, z_grid_size_o = 20,20,20\n",
    "x_grid_size_s, y_grid_size_s, z_grid_size_s = 20,20,20\n",
    "dx_s, dy_s, dz_s, x_left_boundary_s, y_left_boundary_s, z_left_boundary_s = \\\n",
    "                       6/x_grid_size_s, 6/y_grid_size_s, 6/z_grid_size_s, -3, -3, -3\n",
    "dt = 0.005\n",
    "\n",
    "ix = torch.zeros((20,20,20))\n",
    "iy = torch.zeros((20,20,20))\n",
    "iz = torch.zeros((20,20,20))\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        for k in range(20):\n",
    "            ix[i][j][k] = -2.85+(dx_s*i)\n",
    "            iy[i][j][k] = -2.85+(dy_s*i)\n",
    "            iz[i][j][k] = -2.85+(dz_s*i)\n",
    "local_tensor = torch.zeros(1,3,20,20,20)\n",
    "for i in range(1):\n",
    "    local_tensor[i][0] = ix\n",
    "    local_tensor[i][1] = iy\n",
    "    local_tensor[i][2] = iz\n",
    "local_tensor_use = np.transpose(local_tensor,(0,2,3,4,1)).reshape(1,20,20,20,3).to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3ed12-3b2d-4b1a-af28-d4683309d563",
   "metadata": {},
   "source": [
    "### 2.4 Subject of the procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "586b9d46-4e77-4c51-b8c0-4e8b1d108d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerAttentionEM(nn.Module):\n",
    "    def __init__(self, query_EM_dim, mem_source_dim, time_dim, embeded_dim, output_EM_dim, num_heads, num_layers, dropout):\n",
    "   \n",
    "        super(MultiLayerAttentionEM, self).__init__()\n",
    "    \n",
    "        self.num_heads = num_heads\n",
    "        self.embeded_dim = embeded_dim\n",
    "        self.output_EM_dim = output_EM_dim\n",
    "        self.MHembeding = self.num_heads*self.embeded_dim\n",
    "    \n",
    "        self.query = nn.Linear(query_EM_dim, self.MHembeding)\n",
    "        \n",
    "        self.key = nn.Linear(mem_source_dim, self.MHembeding)\n",
    "        \n",
    "        self.value = nn.Linear(query_EM_dim, self.MHembeding)\n",
    "        \n",
    "        self.time = nn.Linear(time_dim, time_dim)\n",
    "        \n",
    "        self.local_rho = nn.Linear(3, mem_source_dim)\n",
    "        self.local_em = nn.Linear(3, query_EM_dim)\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        '''\n",
    "        Check if the code is effective   \n",
    "        '''\n",
    "        self.self_atten_k = nn.Sequential(*[MultiLayerSelfAttention(query_EM_dim, mem_source_dim,\\\n",
    "                                                     embeded_dim, num_heads, num_layers, dropout)\\\n",
    "                             for _ in range(self.num_layers)]).to(torch.device('cuda:0'))\n",
    "        self.self_atten_q = nn.Sequential(*[MultiLayerSelfAttention(query_EM_dim, mem_source_dim, \\\n",
    "                                                   embeded_dim, num_heads, num_layers, dropout) \\\n",
    "                             for _ in range(self.num_layers)]).to(torch.device('cuda:0'))\n",
    "        self.self_atten_v = nn.Sequential(*[MultiLayerSelfAttention(query_EM_dim, mem_source_dim, \\\n",
    "                                                   embeded_dim, num_heads, num_layers, dropout) \\\n",
    "                             for _ in range(self.num_layers)]).to(torch.device('cuda:0'))\n",
    "        self.atten =  nn.Sequential(*[MultiLayerAttention(query_EM_dim, mem_source_dim, \\\n",
    "                                          embeded_dim, num_heads, num_layers, dropout) \\\n",
    "                             for _ in range(self.num_layers)]).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.linear = nn.Linear(embeded_dim * num_heads, output_EM_dim)\n",
    "        \n",
    "        self.self_atten_time = TimeSelfAttention(mem_source_dim, query_EM_dim, time_dim, dropout).to(torch.device('cuda:0'))\n",
    "        \n",
    "        self.atten_local_rho = LocalrhoSelfAttention(mem_source_dim, dropout).to(torch.device('cuda:0'))\n",
    "        self.atten_local_em = LocalemSelfAttention(query_EM_dim, dropout).to(torch.device('cuda:0'))\n",
    "        \n",
    "    def forward(self, source_inputs, EM_inputs, local_tensor_use):\n",
    "        # n_t_steps = source_inputs.shape[1]\n",
    "        n_batch = source_inputs.shape[0]\n",
    "        \n",
    "        # First to time_attention bijkdt to bijkd in order to get the time information\n",
    "        \n",
    "        source_time_attention = self.self_atten_time(self.time(source_inputs), n_batch)\n",
    "        EM_time_attention = self.self_atten_time(self.time(EM_inputs), n_batch)\n",
    "        \n",
    "        source_local_attention = self.atten_local_rho(source_time_attention,self.local_rho(local_tensor_use),source_time_attention,n_batch)\n",
    "        em_local_attention = self.atten_local_em(EM_time_attention,self.local_em(local_tensor_use),EM_time_attention,n_batch)\n",
    "        # print(\"source_time_attention:\",source_time_attention.shape)\n",
    "        # print(\"EM_time_attention:\" ,EM_time_attention.shape)\n",
    "        \n",
    "        # divide the last dimension into multi-heads\n",
    "        q = self.query(em_local_attention).view(*em_local_attention.shape[:-1], self.num_heads, self.embeded_dim)\n",
    "        k = self.key(source_local_attention).view(*source_local_attention.shape[:-1], self.num_heads, self.embeded_dim)\n",
    "        v = self.value(em_local_attention).view(*em_local_attention.shape[:-1], self.num_heads, self.embeded_dim)\n",
    "        # print(q.shape,k.shape,v.shape)\n",
    "        # =======================================================================\n",
    "        # perform multi-layer self-attention along the [nx, ny, nz] dimension for q, k, v\n",
    "        for i_layer in range(self.num_layers):\n",
    "            q = self.self_atten_k[i_layer](q, n_batch)\n",
    "            k = self.self_atten_q[i_layer](k, n_batch)\n",
    "            \n",
    "        \n",
    "        # =======================================================================\n",
    "        for i_layer in range(self.num_layers):\n",
    "            v = self.self_atten_v[i_layer](v, n_batch)\n",
    "            v = self.atten[i_layer](k, q, v, n_batch)    \n",
    "        v = self.linear(v.reshape(*v.shape[:-2], -1))\n",
    "        \n",
    "        output = v\n",
    "        \n",
    "        return output\n",
    "\n",
    "# encoder for rho and J\n",
    "# transformer_encoder_EM = MultiLayerAttentionEM(6, 4, 10, 16, 6, 16, 1, 0.1).to(torch.device('cuda:0'))\n",
    "# transformer_encoder_EM = DataParallel(transformer_encoder_EM, device_ids=list(range(1)))\n",
    "\n",
    "# src_EM = torch.randn(1, nx, ny, nz, 6, 10).to(torch.device('cuda:0'))\n",
    "# tgt = torch.randn(1, nx, ny, nz, 4, 10).to(torch.device('cuda:0'))\n",
    "# output_EM = transformer_encoder_EM(tgt, src_EM,local_tensor_use)\n",
    "# print(tgt.shape, src_EM.shape, output_EM.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180afcca-8781-4ce0-ad0d-00b5009013f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502218e8-3c98-4b37-acd0-4eaefa8208e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([444, 207, 371,  93, 134, 498, 276,  31,  32, 169])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import random\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "index_number = np.arange(25)\n",
    "index_number = random.randint(500, size=(10))\n",
    "index_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8fbf5db-4bbf-46ad-b357-6a6c8447ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model_G, local_tensor_use, batch_size, num_gpus, num_workers):\n",
    "        self.model_G = model_G \n",
    "        self.batch_size = batch_size\n",
    "        self.num_gpus = num_gpus\n",
    "        self.num_workers = num_workers\n",
    "        self.local_tensor_use = local_tensor_use\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # self.Hubercriterion = nn.MSELoss()\n",
    "        # self.Hubercriterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.Hubercriterion = nn.SmoothL1Loss()\n",
    "    def train(self, num_epochs, num_epochs1, learning_rate_G):\n",
    "        # 训练的并行化，通过设置gpu数量来进行并行运算，这里也可以通过设置GPU名称来指定部分GPU进行计算\n",
    "        self.model_G = self.model_G.to(self.device)  \n",
    "\n",
    "        if self.num_gpus > 1:\n",
    "            self.model_G = DataParallel(self.model_G, device_ids=list(range(self.num_gpus)))\n",
    "        \n",
    "        optimizer_G = optim.Adam(self.model_G.parameters(), lr=learning_rate_G)\n",
    "\n",
    "        # scheduler = lr_scheduler.StepLR(optimizer_G, step_size=25, gamma = 0.1)\n",
    "        running_loss_epoch_text = torch.zeros((num_epochs+num_epochs1))\n",
    "        copy_tensor = torch.zeros(32,7,7,7).to(self.device) \n",
    "\n",
    "        for time in range(2):\n",
    "            if time==0:\n",
    "                use_epochs = num_epochs\n",
    "            else:\n",
    "                use_epochs = num_epochs1\n",
    "            for epoch in range(use_epochs):\n",
    "                generator_loss_s = 0.0\n",
    "                discriminator_loss_s = 0.0\n",
    "                number_s = 0\n",
    "                for j in range(4):\n",
    "                    if time==0:\n",
    "                        dataset = MyDataset(190+j)\n",
    "                    else:\n",
    "                        dataset = MyDataset1(190+j)\n",
    "                        self.batch_size = 1\n",
    "                    self.train_loader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers )\n",
    "                    for i, (input1, input2, label) in enumerate(self.train_loader):\n",
    "                        if i < 10:\n",
    "                            continue\n",
    "\n",
    "                        input1, input2, label = input1.to(self.device).reshape(self.batch_size,20,20,20,4,10), \\\n",
    "                                                input2.to(self.device).reshape(self.batch_size,20,20,20,6,10), \\\n",
    "                                                label.to(self.device)  \n",
    "                        \n",
    "                        if time == 0:\n",
    "                            generated_fake = self.model_G(input1, input2,self.local_tensor_use)\n",
    "                        else:\n",
    "                            if i>960:\n",
    "                                middle = input1_last.permute((5,0,1,2,3,4))\n",
    "                                input3 = torch.cat([middle[1:], middle[:1]], dim=0).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "                                input3 = input3.permute((1,2,3,4,5,0))\n",
    "                                input3.permute((5,0,1,2,3,4))[9] = generated_fake.data\n",
    "                                input1_last = input3\n",
    "                                generated_fake =  self.model_G(input1, input3,self.local_tensor_use)\n",
    "                            else:\n",
    "                                generated_fake =  self.model_G(input1, input2,self.local_tensor_use)\n",
    "                                input1_last = input2\n",
    "                            if i%10 == 0 and i!=0:\n",
    "                                input1_last = input2        \n",
    "                        generator_loss = self.Hubercriterion(generated_fake, label)          \n",
    "                        optimizer_G.zero_grad()\n",
    "                        generator_loss.backward()\n",
    "                        optimizer_G.step()\n",
    "                        if i % 500 == 0 and number_s > 1:\n",
    "                            print('[%d, %d, %d] generator_loss: %.5f; ' % (time, epoch + 1, j, generator_loss_s / number_s ))\n",
    "\n",
    "                        generator_loss_s += generator_loss.item()\n",
    "                        number_s += 1\n",
    "                if time==0:\n",
    "                    running_loss_epoch_text[epoch] = generator_loss_s\n",
    "                else:\n",
    "                    running_loss_epoch_text[num_epochs+epoch] = generator_loss_s \n",
    "                      \n",
    "        return running_loss_epoch_text, self.model_G      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c2c16cc-e0d5-4545-add1-331d9a422777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, num):\n",
    "        self.inputs1 = np.load('/data/zhangjunjie/Data/Transfomer-rho-J/10_data_rho_J-'+str(num)+'.npy',allow_pickle=True)\n",
    "        self.inputs2 = np.load('/data/zhangjunjie/Data/Transfomer-EM/10_data_EM-'+str(num)+'.npy',allow_pickle=True)\n",
    "        self.labels = np.load('/data/zhangjunjie/Data/Transfomer-label/10_label-'+str(num)+'.npy',allow_pickle=True)\n",
    "    def __getitem__(self, index):\n",
    "        input1 = torch.Tensor(self.inputs1[index])\n",
    "        input2 = torch.Tensor(self.inputs2[index])\n",
    "        label = torch.Tensor(self.labels[index])\n",
    "        return input1, input2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3217d68-8357-4eb8-bf2d-f9a83a04cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset1(Dataset):\n",
    "    def __init__(self, num):\n",
    "        self.inputs1 = np.load('/data/zhangjunjie/Data/Transfomer-rho-J/10_data_rho_J-'+str(num)+'.npy',allow_pickle=True)\n",
    "        self.inputs2 = np.load('/data/zhangjunjie/Data/Transfomer-EM/10_data_EM-'+str(num)+'.npy',allow_pickle=True)\n",
    "        self.labels = np.load('/data/zhangjunjie/Data/Transfomer-label/10_label-'+str(num)+'.npy',allow_pickle=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input1 = torch.Tensor(self.inputs1[index])\n",
    "        input2 = torch.Tensor(self.inputs2[index])\n",
    "        label = torch.Tensor(self.labels[index])\n",
    "        return input1, input2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c092bae9-8e43-45d8-b9b9-84eb8ca482c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, query_EM_dim,  mem_source_dim, time_dim, embeded_dim, output_EM_dim, num_heads, num_layers, dropout =1, 6, 4, 10, 16, 6, 4, 1, 0.2\n",
    "\n",
    "\n",
    "model_G = MultiLayerAttentionEM(query_EM_dim, mem_source_dim, time_dim, embeded_dim, output_EM_dim, num_heads, num_layers,dropout).to(device)\n",
    "# model_G = torch.load('20231113_200_epochs.pth')\n",
    "# model_G.eval();\n",
    "import time\n",
    "st = time.time()\n",
    "trainer = Trainer(model_G, local_tensor_use, batch_size = batch_size,  num_gpus = 1, num_workers = 8)\n",
    "running_loss_epoch_text, model_S = trainer.train(num_epochs=50, num_epochs1=0, learning_rate_G = 0.0001)\n",
    "print((time.time() - st)/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e19f9066-a160-4043-bb0a-43aad2b76ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid_size_o, y_grid_size_o, z_grid_size_o = 20,20,20\n",
    "x_grid_size_s, y_grid_size_s, z_grid_size_s = 20,20,20\n",
    "dx_s, dy_s, dz_s, x_left_boundary_s, y_left_boundary_s, z_left_boundary_s = \\\n",
    "                       6/x_grid_size_s, 6/y_grid_size_s, 6/z_grid_size_s, -3, -3, -3\n",
    "dt = 0.005\n",
    "\n",
    "ix = torch.zeros((20,20,20))\n",
    "iy = torch.zeros((20,20,20))\n",
    "iz = torch.zeros((20,20,20))\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        for k in range(20):\n",
    "            ix[i][j][k] = -2.85+(dx_s*i)\n",
    "            iy[i][j][k] = -2.85+(dy_s*i)\n",
    "            iz[i][j][k] = -2.85+(dz_s*i)\n",
    "local_tensor = torch.zeros(1,3,20,20,20)\n",
    "for i in range(1):\n",
    "    local_tensor[i][0] = ix\n",
    "    local_tensor[i][1] = iy\n",
    "    local_tensor[i][2] = iz\n",
    "local_tensor_use = np.transpose(local_tensor,(0,2,3,4,1)).reshape(1,20,20,20,3).to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7956fb7-87e7-4e6e-b0fa-c6b4e28acaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "xi, yi = np.mgrid[1:21:1,1:21:1]\n",
    "fig1, axes1 = plt.subplots(ncols=6, nrows=3, figsize = (15,7))\n",
    "lossfunction = nn.SmoothL1Loss()\n",
    "# lossfunction1 = nn.MSELoss()\n",
    "\n",
    "dataset = MyDataset(194)\n",
    "n_batch = 1\n",
    "val_loader = DataLoader(dataset=dataset, batch_size=n_batch, shuffle=True, num_workers=1)\n",
    "generator_loss = 0.0\n",
    "generator_loss_all = 0.0\n",
    "label_plot = torch.zeros((6,20,20))\n",
    "out_plot = torch.zeros((6,20,20))\n",
    "loss = torch.zeros((6,20,20))\n",
    "loss_all = torch.zeros(1920)\n",
    "input_middle = torch.zeros((n_batch,20,20,20,9,10))\n",
    "number = 1000\n",
    "number1 = 0\n",
    "with torch.no_grad():\n",
    "    for x, (input1, input2, label) in enumerate(val_loader):\n",
    "            input_text, input1_text = input1.to(torch.device('cuda:0')).reshape(n_batch,20,20,20,4,10) ,\\\n",
    "            input2.to(torch.device('cuda:0')).reshape(n_batch,20,20,20,6,10)\n",
    "            label_text = label.to(torch.device('cuda:0'))\n",
    "            \n",
    "            if x<800:\n",
    "                continue\n",
    "            # print(input_text.shape,input1_text.shape)  \n",
    "            if x > 960:\n",
    "                middle = input1_last.permute((5,0,1,2,3,4))\n",
    "                input3 = torch.cat([middle[1:], middle[:1]], dim=0).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "                input3 = input3.permute((1,2,3,4,5,0))\n",
    "                input3.permute((5,0,1,2,3,4))[9] = generated_fake.data\n",
    "                input1_last = input3\n",
    "                generated_fake = model_S(input_text, input3, local_tensor_use)\n",
    "            else:\n",
    "                generated_fake = model_S(input_text, input1_text, local_tensor_use)\n",
    "                input1_last = input1_text\n",
    "                \n",
    "            if i%10 ==0 or i!=0:\n",
    "                input1_last = input1_text\n",
    "            generated_fake = model_S(input_text, input1_text, local_tensor_use)    \n",
    "            loss_all[x] = lossfunction(generated_fake, label_text)\n",
    "            \n",
    "            if x == number or x == number+1 or x == number+2 or x == number+3 or x == number+4 or x == number+5:  \n",
    "                m = x%number\n",
    "                # print(m)\n",
    "            # for m in range(6):\n",
    "                for i in range(20):\n",
    "                    for j in range(20):\n",
    "                        label_plot[m][i][j] = label_text[0][5][i][j][2]  #m+number1\n",
    "                        out_plot[m][i][j] = generated_fake[0][5][i][j][2]\n",
    "                        loss[m][i][j] = lossfunction(label_plot[m][i][j],out_plot[m][i][j])\n",
    "                # print(loss.shape)\n",
    "                axes1[0,m].pcolormesh(xi, yi, label_plot[m].detach().numpy(),cmap ='viridis')\n",
    "                axes1[1,m].pcolormesh(xi, yi, out_plot[m].detach().numpy(),cmap ='viridis')\n",
    "                axes1[2,m].pcolormesh(xi, yi, loss[m].detach().numpy(),cmap ='RdBu')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
